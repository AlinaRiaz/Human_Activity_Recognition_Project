{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PjZk7NTmwQkyD8obK6FGsWUa_oVH5-nH","timestamp":1766005266838}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **STEP 0** : Setup"],"metadata":{"id":"SyKG6nhs1_WQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"MGkScQviIrOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","import random\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"0D8nIaZ-2M0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"TensorFlow Version:\", tf.__version__)\n","print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRPEHHb_2bWY","executionInfo":{"status":"ok","timestamp":1765994824641,"user_tz":-300,"elapsed":38,"user":{"displayName":"Alina Riaz","userId":"17786826717857772216"}},"outputId":"5bfb71f4-50e4-481f-9e22-5945003d38ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow Version: 2.19.0\n","GPU Available: []\n"]}]},{"cell_type":"code","source":["# Path to the dataset (change according to your Colab setup)\n","DATASET_DIR = '/content/dataset/'\n","\n","# Define frame size for CNN input\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","IMG_CHANNELS = 3\n","\n","# Number of frames to sample per video\n","FRAMES_PER_VIDEO = 20\n","\n","# Random seed for reproducibility\n","SEED = 42\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","random.seed(SEED)\n","\n","print(\"Environment setup complete.\")\n","print(f\"Dataset path: {DATASET_DIR}\")\n","print(f\"Image size: {IMG_HEIGHT}x{IMG_WIDTH}x{IMG_CHANNELS}\")\n","print(f\"Frames per video: {FRAMES_PER_VIDEO}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLCaeG2j2wJn","executionInfo":{"status":"ok","timestamp":1765994824684,"user_tz":-300,"elapsed":39,"user":{"displayName":"Alina Riaz","userId":"17786826717857772216"}},"outputId":"d4d71004-2271-4704-ebc0-cb46ce6f8e59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Environment setup complete.\n","Dataset path: /content/dataset/\n","Image size: 224x224x3\n","Frames per video: 20\n"]}]},{"cell_type":"markdown","source":["# STEP 1 : Import of Data set and Extraction"],"metadata":{"id":"QeY-GHHz3ZGI"}},{"cell_type":"code","source":["import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import zipfile"],"metadata":{"id":"YlfuHn5V7NiZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQgRf6IMdrFx","executionInfo":{"status":"ok","timestamp":1765994828386,"user_tz":-300,"elapsed":3636,"user":{"displayName":"Alina Riaz","userId":"17786826717857772216"}},"outputId":"c819fcc0-758b-4f78-c901-edad1050943e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'ucf101-action-recognition' dataset.\n","Path to dataset files: /kaggle/input/ucf101-action-recognition\n"]}]},{"cell_type":"code","source":["import os\n","\n","# STEP 1: Dataset Setup\n","# 'path' variable is the dataset folder returned by kagglehub.dataset_download\n","DATASET_DIR = \"/root/.cache/kagglehub/datasets/matthewjansen/ucf101-action-recognition/versions/4\"\n","\n","# Inspect the contents\n","all_items = os.listdir(DATASET_DIR)\n","print(\"All items in dataset directory:\", all_items)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nJQ2K44hSJ7","executionInfo":{"status":"ok","timestamp":1765994828404,"user_tz":-300,"elapsed":13,"user":{"displayName":"Alina Riaz","userId":"17786826717857772216"}},"outputId":"141d7615-cb4e-4a72-ffe0-a5a4ee78fff4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All items in dataset directory: ['val', 'val.csv', 'test.csv', 'train.csv', 'train', 'test']\n"]}]},{"cell_type":"markdown","source":["# Part A : Classical Machine Learing"],"metadata":{"id":"JT3KCO7RNZse"}},{"cell_type":"markdown","source":["Step A1:"],"metadata":{"id":"d5YVsQUiNqsH"}},{"cell_type":"code","source":["NUM_FRAMES = 8\n","FRAME_SIZE = (112, 112)\n"],"metadata":{"id":"iYPixfC9Nvz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A1.1 — Imports\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm"],"metadata":{"id":"cp40_EY7OGh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A1.2 — Frame Sampling Function\n","def sample_frames(video_path, num_frames=8, size=(112, 112)):\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    if total_frames == 0:\n","        cap.release()\n","        return None\n","\n","    frame_indices = np.linspace(0, total_frames - 1, num_frames).astype(int)\n","    frames = []\n","\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        frame = cv2.resize(frame, size)\n","        frames.append(frame)\n","\n","    cap.release()\n","\n","    if len(frames) < num_frames:\n","        return None\n","\n","    return np.array(frames)"],"metadata":{"id":"bfqI0FzGOQHn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A1.3 — Feature Extraction from Frames\n","def extract_motion_features(frames):\n","    features = []\n","\n","    # Appearance features\n","    features.append(frames.mean())\n","    features.append(frames.std())\n","\n","    # Motion features (frame differences)\n","    diffs = np.abs(np.diff(frames, axis=0))\n","    features.append(diffs.mean())\n","    features.append(diffs.std())\n","\n","    return np.array(features)"],"metadata":{"id":"sJYzNedAOaAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A1.4 — Build Dataset Features (TRAIN / VAL / TEST)\n","def build_feature_dataset(df, root_path):\n","    X = []\n","    y = []\n","\n","    for _, row in tqdm(df.iterrows(), total=len(df)):\n","        video_path = os.path.join(root_path, row['clip_path'].lstrip('/'))\n","\n","        frames = sample_frames(video_path, NUM_FRAMES, FRAME_SIZE)\n","        if frames is None:\n","            continue\n","\n","        feats = extract_motion_features(frames)\n","        X.append(feats)\n","        y.append(row['label'])\n","\n","    return np.array(X), np.array(y)"],"metadata":{"id":"y4ReTi3JOkSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A1.5 — Run Feature Extraction\n","\n","X_train, y_train = build_feature_dataset(train_df, path)\n","X_val, y_val     = build_feature_dataset(val_df, path)\n","X_test, y_test   = build_feature_dataset(test_df, path)\n","\n","print(\"Train features shape:\", X_train.shape)\n","print(\"Val features shape:\", X_val.shape)\n","print(\"Test features shape:\", X_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NgRPIY9gOpEd","executionInfo":{"status":"ok","timestamp":1765996372210,"user_tz":-300,"elapsed":1543725,"user":{"displayName":"Alina Riaz","userId":"17786826717857772216"}},"outputId":"b291bfd9-c7a0-4ced-c688-f50f0e9a427f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10055/10055 [19:12<00:00,  8.73it/s]\n","100%|██████████| 1673/1673 [03:14<00:00,  8.61it/s]\n","100%|██████████| 1723/1723 [03:16<00:00,  8.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Train features shape: (10012, 4)\n","Val features shape: (1669, 4)\n","Test features shape: (1715, 4)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["Step A2:"],"metadata":{"id":"Ei2S9I6cPBAm"}},{"cell_type":"code","source":["#STEP A2.1 — Encode Labels (IMPORTANT)\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","\n","y_train_enc = le.fit_transform(y_train)\n","y_val_enc   = le.transform(y_val)\n","y_test_enc  = le.transform(y_test)\n","\n","print(\"Class mapping:\")\n","for i, c in enumerate(le.classes_):\n","    print(i, \"->\", c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjGTrrDjPDKQ","executionInfo":{"status":"ok","timestamp":1765996372255,"user_tz":-300,"elapsed":22,"user":{"displayName":"Alina Riaz","userId":"17786826717857772216"}},"outputId":"b547cf30-af01-47f8-c8b1-512f6ff0a8ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class mapping:\n","0 -> ApplyEyeMakeup\n","1 -> ApplyLipstick\n","2 -> Archery\n","3 -> BabyCrawling\n","4 -> BalanceBeam\n","5 -> BandMarching\n","6 -> BaseballPitch\n","7 -> Basketball\n","8 -> BasketballDunk\n","9 -> BenchPress\n","10 -> Biking\n","11 -> Billiards\n","12 -> BlowDryHair\n","13 -> BlowingCandles\n","14 -> BodyWeightSquats\n","15 -> Bowling\n","16 -> BoxingPunchingBag\n","17 -> BoxingSpeedBag\n","18 -> BreastStroke\n","19 -> BrushingTeeth\n","20 -> CleanAndJerk\n","21 -> CliffDiving\n","22 -> CricketBowling\n","23 -> CricketShot\n","24 -> CuttingInKitchen\n","25 -> Diving\n","26 -> Drumming\n","27 -> Fencing\n","28 -> FieldHockeyPenalty\n","29 -> FloorGymnastics\n","30 -> FrisbeeCatch\n","31 -> FrontCrawl\n","32 -> GolfSwing\n","33 -> Haircut\n","34 -> HammerThrow\n","35 -> Hammering\n","36 -> HandstandPushups\n","37 -> HandstandWalking\n","38 -> HeadMassage\n","39 -> HighJump\n","40 -> HorseRace\n","41 -> HorseRiding\n","42 -> HulaHoop\n","43 -> IceDancing\n","44 -> JavelinThrow\n","45 -> JugglingBalls\n","46 -> JumpRope\n","47 -> JumpingJack\n","48 -> Kayaking\n","49 -> Knitting\n","50 -> LongJump\n","51 -> Lunges\n","52 -> MilitaryParade\n","53 -> Mixing\n","54 -> MoppingFloor\n","55 -> Nunchucks\n","56 -> ParallelBars\n","57 -> PizzaTossing\n","58 -> PlayingCello\n","59 -> PlayingDaf\n","60 -> PlayingDhol\n","61 -> PlayingFlute\n","62 -> PlayingGuitar\n","63 -> PlayingPiano\n","64 -> PlayingSitar\n","65 -> PlayingTabla\n","66 -> PlayingViolin\n","67 -> PoleVault\n","68 -> PommelHorse\n","69 -> PullUps\n","70 -> Punch\n","71 -> PushUps\n","72 -> Rafting\n","73 -> RockClimbingIndoor\n","74 -> RopeClimbing\n","75 -> Rowing\n","76 -> SalsaSpin\n","77 -> ShavingBeard\n","78 -> Shotput\n","79 -> SkateBoarding\n","80 -> Skiing\n","81 -> Skijet\n","82 -> SkyDiving\n","83 -> SoccerJuggling\n","84 -> SoccerPenalty\n","85 -> StillRings\n","86 -> SumoWrestling\n","87 -> Surfing\n","88 -> Swing\n","89 -> TableTennisShot\n","90 -> TaiChi\n","91 -> TennisSwing\n","92 -> ThrowDiscus\n","93 -> TrampolineJumping\n","94 -> Typing\n","95 -> UnevenBars\n","96 -> VolleyballSpiking\n","97 -> WalkingWithDog\n","98 -> WallPushups\n","99 -> WritingOnBoard\n","100 -> YoYo\n"]}]},{"cell_type":"code","source":["#Step A2.2 - SVM + GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","\n","svm = SVC()\n","\n","svm_params = {\n","    'C': [0.1, 1, 10],\n","    'kernel': ['rbf', 'linear'],\n","    'gamma': ['scale', 'auto']\n","}\n","\n","svm_grid = GridSearchCV(\n","    svm,\n","    svm_params,\n","    cv=5,\n","    scoring='f1_macro',\n","    n_jobs=-1\n",")\n","\n","svm_grid.fit(X_train, y_train_enc)\n","\n","print(\"Best SVM params:\", svm_grid.best_params_)"],"metadata":{"id":"ap8pgrxsPUQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluate SVM on Test Set\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","svm_best = svm_grid.best_estimator_\n","svm_preds = svm_best.predict(X_test)\n","\n","print(\"SVM Classification Report:\")\n","print(classification_report(y_test_enc, svm_preds, target_names=le.classes_))"],"metadata":{"id":"IdT-H9kFPcpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A2.3 — Model 2: Random Forest\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rf = RandomForestClassifier(random_state=42)\n","\n","rf_params = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [None, 5, 10]\n","}\n","\n","rf_grid = GridSearchCV(\n","    rf,\n","    rf_params,\n","    cv=5,\n","    scoring='f1_macro',\n","    n_jobs=-1\n",")\n","\n","rf_grid.fit(X_train, y_train_enc)\n","\n","print(\"Best RF params:\", rf_grid.best_params_)"],"metadata":{"id":"1TrpgpluPjY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluate RF on Test Set\n","rf_best = rf_grid.best_estimator_\n","rf_preds = rf_best.predict(X_test)\n","\n","print(\"Random Forest Classification Report:\")\n","print(classification_report(y_test_enc, rf_preds, target_names=le.classes_))"],"metadata":{"id":"XNcVTLtbPzNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A2.4 — Feature Importance (VERY IMPORTANT)\n","import matplotlib.pyplot as plt\n","\n","feature_names = [\n","    'mean_intensity',\n","    'std_intensity',\n","    'mean_motion',\n","    'std_motion'\n","]\n","\n","importances = rf_best.feature_importances_\n","\n","plt.bar(feature_names, importances)\n","plt.title(\"Feature Importance (Random Forest)\")\n","plt.ylabel(\"Importance\")\n","plt.show()"],"metadata":{"id":"w62BoLZEP6Rf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step A3:"],"metadata":{"id":"eZxDdbvatb4m"}},{"cell_type":"code","source":["#step A3.1\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","def evaluate_model(y_true, y_pred, model_name):\n","    print(f\"--- {model_name} Evaluation ---\")\n","    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n","    print(\"F1 Macro:\", f1_score(y_true, y_pred, average='macro'))\n","    print(\"Precision Macro:\", precision_score(y_true, y_pred, average='macro'))\n","    print(\"Recall Macro:\", recall_score(y_true, y_pred, average='macro'))\n","    print(\"\\n\")\n"],"metadata":{"id":"Uv97wWEptd6_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_model(y_test_enc, svm_preds, \"SVM\")\n","evaluate_model(y_test_enc, rf_preds, \"Random Forest\")\n"],"metadata":{"id":"6gsSj2nQtk6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A3.2 — Confusion Matrix\n","\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","cm = confusion_matrix(y_test_enc, dl_preds)\n","plt.figure(figsize=(12,10))\n","sns.heatmap(cm, cmap=\"Blues\", annot=False)\n","plt.title(\"CNN+LSTM Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()"],"metadata":{"id":"KyYSo7qmtx45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP A3.3 — Statistical Significance Testing (Optional, Advanced)\n","\n","from mlxtend.evaluate import paired_ttest_5x2cv\n","# X_test_features, y_test_enc\n","t, p = paired_ttest_5x2cv(estimator1=rf_best, estimator2=dl_model, X=X_test, y=y_test_enc)\n","print(\"Paired t-test result: t =\", t, \", p =\", p)"],"metadata":{"id":"j7iijxG7t69K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part A : Classical Machine Learing"],"metadata":{"id":"p89ttEnVvL6M"}},{"cell_type":"code","source":["#STEP B1 — Select Subset of Classes\n","\n","selected_classes = [\n","    \"Basketball\", \"TennisSwing\", \"SoccerJuggling\", \"JumpRope\",\n","    \"Swing\", \"WalkingWithDog\", \"Kayaking\", \"HighJump\", \"PullUps\"\n","]"],"metadata":{"id":"A6o4DC69vOKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP B2 — Prepare DataFrame Subset\n","\n","train_df_subset = train_df[train_df['label'].isin(selected_classes)].reset_index(drop=True)\n","val_df_subset = val_df[val_df['label'].isin(selected_classes)].reset_index(drop=True)\n","test_df_subset = test_df[test_df['label'].isin(selected_classes)].reset_index(drop=True)"],"metadata":{"id":"ABeq0dgfvS9-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP B3 — Frame Extraction\n","\n","import cv2\n","import numpy as np\n","\n","def extract_frames(video_path, max_frames=16):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    step = max(1, total_frames // max_frames)\n","    for i in range(max_frames):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i*step)\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (64,64)) / 255.0  # normalize\n","        frames.append(frame)\n","    cap.release()\n","    # Pad frames if less than max_frames\n","    while len(frames) < max_frames:\n","        frames.append(np.zeros((64,64,3)))\n","    return np.array(frames)"],"metadata":{"id":"nBR9S2oFvXwh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP B4 — Encode Labels\n","\n","#Classification label: Movement exists or not\n","\n","train_df_subset['movement'] = 1  # All selected classes → movement\n","\n","\n","#Regression label: Movement type index (for LSTM to predict)\n","\n","label_map = {cls:i for i, cls in enumerate(selected_classes)}\n","train_df_subset['movement_type'] = train_df_subset['label'].map(label_map)"],"metadata":{"id":"SaWhjYBmvgNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP B5 — Create Data Generator\n","\n","import tensorflow as tf\n","\n","class VideoDataGenerator(tf.keras.utils.Sequence):\n","    def __init__(self, df, batch_size=8, max_frames=16, shuffle=True):\n","        self.df = df\n","        self.batch_size = batch_size\n","        self.max_frames = max_frames\n","        self.shuffle = shuffle\n","        self.indexes = np.arange(len(df))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.df)/self.batch_size))\n","\n","    def __getitem__(self, idx):\n","        batch_idx = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n","        batch_df = self.df.iloc[batch_idx]\n","        X = np.array([extract_frames(f\"/content/dataset{row['clip_path']}\", self.max_frames) for _, row in batch_df.iterrows()])\n","        y_class = np.array(batch_df['movement'])\n","        y_reg = np.array(batch_df['movement_type'])\n","        return X, {\"classification\": y_class, \"regression\": y_reg}\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)"],"metadata":{"id":"ByuyjOuLvpsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP B6 — Build CNN+LSTM Model\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, Dropout\n","\n","max_frames, H, W, C = 16, 64, 64, 3\n","\n","# Base CNN (pretrained)\n","cnn_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(H,W,C), pooling='avg')\n","\n","for layer in cnn_base.layers:\n","    layer.trainable = False  # freeze CNN\n","\n","# Video Input\n","video_input = Input(shape=(max_frames, H, W, C))\n","x = TimeDistributed(cnn_base)(video_input)\n","x = LSTM(128)(x)\n","x = Dropout(0.5)(x)\n","\n","# Dual Output: classification + regression\n","class_output = Dense(1, activation='sigmoid', name='classification')(x)  # movement yes/no\n","reg_output = Dense(len(selected_classes), activation='softmax', name='regression')(x)  # movement type\n","\n","model = Model(inputs=video_input, outputs=[class_output, reg_output])\n","model.compile(optimizer='adam',\n","              loss={'classification':'binary_crossentropy', 'regression':'sparse_categorical_crossentropy'},\n","              metrics={'classification':'accuracy', 'regression':'accuracy'})\n","model.summary()"],"metadata":{"id":"jzcUkpxuvs3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP B7 — Train the Model\n","train_gen = VideoDataGenerator(train_df_subset, batch_size=8)\n","val_gen = VideoDataGenerator(val_df_subset, batch_size=8)\n","\n","history = model.fit(train_gen, validation_data=val_gen, epochs=3)"],"metadata":{"id":"N1Drvajxv0t6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP B8 — Evaluate on Test Set\n","test_gen = VideoDataGenerator(test_df_subset, batch_size=8, shuffle=False)\n","results = model.evaluate(test_gen)\n","print(\"Test Loss & Accuracy:\", results)"],"metadata":{"id":"PO__HIH4v48I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluation"],"metadata":{"id":"2rprpCjcwP4J"}},{"cell_type":"code","source":["#STEP 1 — Plot Training Curves\n","import matplotlib.pyplot as plt\n","\n","def plot_training(history, metric='accuracy'):\n","    # Classification\n","    plt.figure(figsize=(12,5))\n","    plt.subplot(1,2,1)\n","    plt.plot(history.history['classification_accuracy'], label='train')\n","    plt.plot(history.history['val_classification_accuracy'], label='val')\n","    plt.title('Classification Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.subplot(1,2,2)\n","    plt.plot(history.history['regression_accuracy'], label='train')\n","    plt.plot(history.history['val_regression_accuracy'], label='val')\n","    plt.title('Regression (Movement Type) Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.show()\n","\n","plot_training(history)"],"metadata":{"id":"ljWDGHf1wSf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP 2 — Evaluate on Test Set\n","test_gen = VideoDataGenerator(test_df_subset, batch_size=8, shuffle=False)\n","results = model.evaluate(test_gen)\n","print(\"Test Loss & Accuracy (classification, regression):\", results)"],"metadata":{"id":"FZoYZe1UwWiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP 3 — Predictions on Test Set\n","import numpy as np\n","\n","y_true_class = []\n","y_true_reg = []\n","y_pred_class = []\n","y_pred_reg = []\n","\n","for X, y in test_gen:\n","    preds_class, preds_reg = model.predict(X)\n","    y_true_class.extend(y['classification'])\n","    y_true_reg.extend(y['regression'])\n","    y_pred_class.extend((preds_class > 0.5).astype(int).flatten())\n","    y_pred_reg.extend(np.argmax(preds_reg, axis=1))"],"metadata":{"id":"irTcKtqPwa3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP 4 — Classification Metrics\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","print(\"--- Movement Detection (Classification) ---\")\n","print(\"Accuracy:\", accuracy_score(y_true_class, y_pred_class))\n","print(\"F1 Macro:\", f1_score(y_true_class, y_pred_class, average='macro'))\n","print(\"Precision Macro:\", precision_score(y_true_class, y_pred_class, average='macro'))\n","print(\"Recall Macro:\", recall_score(y_true_class, y_pred_class, average='macro'))"],"metadata":{"id":"fjhaXfiZweUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP 5 — Regression Metrics (Movement Type)\n","print(\"--- Movement Type (Regression / Multi-class Classification) ---\")\n","print(\"Accuracy:\", accuracy_score(y_true_reg, y_pred_reg))\n","print(\"F1 Macro:\", f1_score(y_true_reg, y_pred_reg, average='macro'))"],"metadata":{"id":"R94HEsDRwhro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#STEP 6 — Confusion Matrix for Movement Types\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","cm = confusion_matrix(y_true_reg, y_pred_reg)\n","plt.figure(figsize=(12,10))\n","sns.heatmap(cm, cmap=\"Blues\", annot=False)\n","plt.title(\"Movement Type Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()"],"metadata":{"id":"G02VTz6Gwjgr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Comparative Analysis"],"metadata":{"id":"ZlTIafBUxLCi"}},{"cell_type":"code","source":["#Step 1 — Collect Metrics\n","\n","# Classical ML\n","metrics_classical = {\n","    \"RandomForest\": {\n","        \"Accuracy\": accuracy_score(y_test, y_pred_rf),\n","        \"F1_macro\": f1_score(y_test, y_pred_rf, average='macro')\n","    },\n","    \"SVM\": {\n","        \"Accuracy\": accuracy_score(y_test, y_pred_svm),\n","        \"F1_macro\": f1_score(y_test, y_pred_svm, average='macro')\n","    }\n","}\n","\n","# Deep Learning\n","metrics_deep = {\n","    \"CNN_LSTM\": {\n","        \"Classification Accuracy\": accuracy_score(y_true_class, y_pred_class),\n","        \"Classification F1_macro\": f1_score(y_true_class, y_pred_class, average='macro'),\n","        \"Regression Accuracy\": accuracy_score(y_true_reg, y_pred_reg),\n","        \"Regression F1_macro\": f1_score(y_true_reg, y_pred_reg, average='macro')\n","    }\n","}"],"metadata":{"id":"sMql_JAtxOQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 2 — Summary Table\n","import pandas as pd\n","\n","summary = pd.DataFrame({\n","    \"Model\": [\"Random Forest\", \"SVM\", \"CNN+LSTM (Movement Detection)\", \"CNN+LSTM (Movement Type)\"],\n","    \"Accuracy\": [\n","        metrics_classical[\"RandomForest\"][\"Accuracy\"],\n","        metrics_classical[\"SVM\"][\"Accuracy\"],\n","        metrics_deep[\"CNN_LSTM\"][\"Classification Accuracy\"],\n","        metrics_deep[\"CNN_LSTM\"][\"Regression Accuracy\"]\n","    ],\n","    \"F1 Macro\": [\n","        metrics_classical[\"RandomForest\"][\"F1_macro\"],\n","        metrics_classical[\"SVM\"][\"F1_macro\"],\n","        metrics_deep[\"CNN_LSTM\"][\"Classification F1_macro\"],\n","        metrics_deep[\"CNN_LSTM\"][\"Regression F1_macro\"]\n","    ]\n","})\n","\n","print(summary)"],"metadata":{"id":"oz28lAVPxWwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 3 — Visualization\n","import matplotlib.pyplot as plt\n","\n","# Accuracy comparison\n","plt.figure(figsize=(8,5))\n","plt.bar(summary[\"Model\"], summary[\"Accuracy\"], color=['skyblue','orange','green','red'])\n","plt.title(\"Model Accuracy Comparison\")\n","plt.ylabel(\"Accuracy\")\n","plt.xticks(rotation=30)\n","plt.ylim(0,1)\n","plt.show()\n","\n","# F1 Macro comparison\n","plt.figure(figsize=(8,5))\n","plt.bar(summary[\"Model\"], summary[\"F1 Macro\"], color=['skyblue','orange','green','red'])\n","plt.title(\"Model F1 Macro Comparison\")\n","plt.ylabel(\"F1 Score (Macro)\")\n","plt.xticks(rotation=30)\n","plt.ylim(0,1)\n","plt.show()"],"metadata":{"id":"ZFeAXH9Gxe11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf /content/Human_Activity_Recognition_Project\n","\n"],"metadata":{"id":"sRz6zrUD5PX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/AlinaRiaz/Human_Activity_Recognition_Project.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FF4mCtmIBws1","executionInfo":{"status":"ok","timestamp":1766003777176,"user_tz":-300,"elapsed":737,"user":{"displayName":"Alina Naqvi","userId":"04142602440198484269"}},"outputId":"ab451058-a312-40c2-85f6-c896c76bd601"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Human_Activity_Recognition_Project'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (3/3), done.\n"]}]},{"cell_type":"code","source":["%cd /content/Human_Activity_Recognition_Project\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0tghetvB18k","executionInfo":{"status":"ok","timestamp":1766003797737,"user_tz":-300,"elapsed":44,"user":{"displayName":"Alina Naqvi","userId":"04142602440198484269"}},"outputId":"b49652b9-dd81-43d4-ff1f-69ba378b7ffc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Human_Activity_Recognition_Project\n"]}]},{"cell_type":"code","source":["!ls /content\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNFNQj-5CWiE","executionInfo":{"status":"ok","timestamp":1766003985806,"user_tz":-300,"elapsed":116,"user":{"displayName":"Alina Naqvi","userId":"04142602440198484269"}},"outputId":"0da53d68-5598-4833-fbb7-76601438213b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Human_Activity_Recognition_Project\n"]}]},{"cell_type":"code","source":["%cd /content/Human_Activity_Recognition_Project\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfF7bix_B7iW","executionInfo":{"status":"ok","timestamp":1766005054819,"user_tz":-300,"elapsed":66,"user":{"displayName":"Alina Naqvi","userId":"04142602440198484269"}},"outputId":"c43c08d4-2484-4b1a-bf3c-28f547ee40ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Human_Activity_Recognition_Project\n"]}]}]}